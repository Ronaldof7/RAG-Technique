{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![](https://europe-west1-atp-views-tracker.cloudfunctions.net/working-analytics?notebook=adaptive-retrieval) -->\n",
    "\n",
    "\n",
    "\n",
    "# Adaptive Retrieval-Augmented Generation (RAG) System\n",
    "\n",
    "## Overview\n",
    "\n",
    "This system implements an advanced Retrieval-Augmented Generation (RAG) approach that adapts its retrieval strategy based on the type of query. By leveraging Language Models (LLMs) at various stages, it aims to provide more accurate, relevant, and context-aware responses to user queries.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Traditional RAG systems often use a one-size-fits-all approach to retrieval, which can be suboptimal for different types of queries. Our adaptive system is motivated by the understanding that different types of questions require different retrieval strategies. For example, a factual query might benefit from precise, focused retrieval, while an analytical query might require a broader, more diverse set of information.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "1. **Query Classifier**: Determines the type of query (Factual, Analytical, Opinion, or Contextual).\n",
    "\n",
    "2. **Adaptive Retrieval Strategies**: Four distinct strategies tailored to different query types:\n",
    "   - Factual Strategy\n",
    "   - Analytical Strategy\n",
    "   - Opinion Strategy\n",
    "   - Contextual Strategy\n",
    "\n",
    "3. **LLM Integration**: LLMs are used throughout the process to enhance retrieval and ranking.\n",
    "\n",
    "4. **OpenAI GPT Model**: Generates the final response using the retrieved documents as context.\n",
    "\n",
    "## Method Details\n",
    "\n",
    "### 1. Query Classification\n",
    "\n",
    "The system begins by classifying the user's query into one of four categories:\n",
    "- Factual: Queries seeking specific, verifiable information.\n",
    "- Analytical: Queries requiring comprehensive analysis or explanation.\n",
    "- Opinion: Queries about subjective matters or seeking diverse viewpoints.\n",
    "- Contextual: Queries that depend on user-specific context.\n",
    "\n",
    "### 2. Adaptive Retrieval Strategies\n",
    "\n",
    "Each query type triggers a specific retrieval strategy:\n",
    "\n",
    "#### Factual Strategy\n",
    "- Enhances the original query using an LLM for better precision.\n",
    "- Retrieves documents based on the enhanced query.\n",
    "- Uses an LLM to rank documents by relevance.\n",
    "\n",
    "#### Analytical Strategy\n",
    "- Generates multiple sub-queries using an LLM to cover different aspects of the main query.\n",
    "- Retrieves documents for each sub-query.\n",
    "- Ensures diversity in the final document selection using an LLM.\n",
    "\n",
    "#### Opinion Strategy\n",
    "- Identifies different viewpoints on the topic using an LLM.\n",
    "- Retrieves documents representing each viewpoint.\n",
    "- Uses an LLM to select a diverse range of opinions from the retrieved documents.\n",
    "\n",
    "#### Contextual Strategy\n",
    "- Incorporates user-specific context into the query using an LLM.\n",
    "- Performs retrieval based on the contextualized query.\n",
    "- Ranks documents considering both relevance and user context.\n",
    "\n",
    "### 3. LLM-Enhanced Ranking\n",
    "\n",
    "After retrieval, each strategy uses an LLM to perform a final ranking of the documents. This step ensures that the most relevant and appropriate documents are selected for the next stage.\n",
    "\n",
    "### 4. Response Generation\n",
    "\n",
    "The final set of retrieved documents is passed to an OpenAI GPT model, which generates a response based on the query and the provided context.\n",
    "\n",
    "## Benefits of This Approach\n",
    "\n",
    "1. **Improved Accuracy**: By tailoring the retrieval strategy to the query type, the system can provide more accurate and relevant information.\n",
    "\n",
    "2. **Flexibility**: The system adapts to different types of queries, handling a wide range of user needs.\n",
    "\n",
    "3. **Context-Awareness**: Especially for contextual queries, the system can incorporate user-specific information for more personalized responses.\n",
    "\n",
    "4. **Diverse Perspectives**: For opinion-based queries, the system actively seeks out and presents multiple viewpoints.\n",
    "\n",
    "5. **Comprehensive Analysis**: The analytical strategy ensures a thorough exploration of complex topics.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This adaptive RAG system represents a significant advancement over traditional RAG approaches. By dynamically adjusting its retrieval strategy and leveraging LLMs throughout the process, it aims to provide more accurate, relevant, and nuanced responses to a wide variety of user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/adaptive_retrieval.svg\" alt=\"adaptive retrieval\" style=\"width:100%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Installation and Imports\n",
    "\n",
    "The cell below installs all necessary packages required to run this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in ./myenv/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: langchain in ./myenv/lib/python3.12/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-openai in ./myenv/lib/python3.12/site-packages (0.3.6)\n",
      "Requirement already satisfied: python-dotenv in ./myenv/lib/python3.12/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in ./myenv/lib/python3.12/site-packages (from faiss-cpu) (2.1.3)\n",
      "Requirement already satisfied: packaging in ./myenv/lib/python3.12/site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in ./myenv/lib/python3.12/site-packages (from langchain) (0.3.75)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in ./myenv/lib/python3.12/site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in ./myenv/lib/python3.12/site-packages (from langchain) (0.4.21)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./myenv/lib/python3.12/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./myenv/lib/python3.12/site-packages (from langchain) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in ./myenv/lib/python3.12/site-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./myenv/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./myenv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./myenv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./myenv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./myenv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./myenv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./myenv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./myenv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in ./myenv/lib/python3.12/site-packages (from langchain-openai) (1.63.2)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in ./myenv/lib/python3.12/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./myenv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./myenv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./myenv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./myenv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in ./myenv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./myenv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: httpcore==1.* in ./myenv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./myenv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./myenv/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2025.9.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in ./myenv/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./myenv/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./myenv/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install faiss-cpu langchain langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from typing import Dict, Any, List\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the query classifer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryClassifier:\n",
    "    def __init__(self, model_name=\"llama3\"):\n",
    "        self.llm = OllamaLLM(model=model_name)\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"query\"],\n",
    "            template=(\n",
    "                \"Classify the following query into one of these categories: \"\n",
    "                \"Factual, Analytical, Opinion, or Contextual.\\nQuery: {query}\\nAnswer only with the category:\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def classify(self, query: str) -> str:\n",
    "        print(\"Classifying query...\")\n",
    "        formatted_prompt = self.prompt.format(query=query)\n",
    "        raw_output = self.llm.invoke(formatted_prompt)\n",
    "\n",
    "        # Extract only the first matching category\n",
    "        for cat in [\"Factual\", \"Analytical\", \"Opinion\", \"Contextual\"]:\n",
    "            if cat.lower() in raw_output.lower():\n",
    "                return cat\n",
    "\n",
    "        # Fallback in case LLM output is unexpected\n",
    "        raise ValueError(f\"Could not classify query. LLM output: {raw_output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Base Retriever class, such that the complex ones will inherit from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRetrievalStrategy:\n",
    "    def __init__(self, texts):\n",
    "        self.embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=0)\n",
    "        self.documents = text_splitter.create_documents(texts)\n",
    "        self.db = FAISS.from_documents(self.documents, self.embeddings)\n",
    "        self.llm = OllamaLLM(model=\"llama3\")\n",
    "\n",
    "\n",
    "    def retrieve(self, query, k=4):\n",
    "        return self.db.similarity_search(query, k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Factual retriever strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelevantScore(BaseModel):\n",
    "    score: float = Field(description=\"The relevance score of the document to the query\", example=8.0)\n",
    "\n",
    "class FactualRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def __init__(self, db, model_name=\"llama3\"):\n",
    "        self.db = db\n",
    "        self.llm = OllamaLLM(model=model_name)\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 4) -> List:\n",
    "        print(\"retrieving factual\")\n",
    "\n",
    "        # Step 1: Enhance the query using the LLM\n",
    "        enhanced_query_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\"],\n",
    "            template=\"Enhance this factual query for better information retrieval: {query}\"\n",
    "        )\n",
    "        enhanced_query = self.llm.invoke(enhanced_query_prompt.format(query=query))\n",
    "        print(f\"Enhanced query: {enhanced_query}\")\n",
    "\n",
    "        # Step 2: Retrieve documents using the enhanced query\n",
    "        docs = self.db.similarity_search(enhanced_query, k=k*2)\n",
    "\n",
    "        # Step 3: Rank documents using the LLM\n",
    "        ranking_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"doc\"],\n",
    "            template=\"On a scale of 1-10, how relevant is this document to the query: '{query}'?\\nDocument: {doc}\\nRelevance score:\"\n",
    "        )\n",
    "\n",
    "        ranked_docs = []\n",
    "        print(\"ranking docs\")\n",
    "        for doc in docs:\n",
    "            input_text = ranking_prompt.format(query=enhanced_query, doc=doc.page_content)\n",
    "            score_output = self.llm.invoke(input_text, output_class=RelevantScore)\n",
    "            score = float(score_output.score)\n",
    "            ranked_docs.append((doc, score))\n",
    "\n",
    "        # Step 4: Sort by relevance and return top k\n",
    "        ranked_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [doc for doc, _ in ranked_docs[:k]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Analytical retriever strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectedIndices(BaseModel):\n",
    "    indices: List[int] = Field(description=\"Indices of selected documents\", example=[0, 1, 2, 3])\n",
    "\n",
    "class SubQueries(BaseModel):\n",
    "    sub_queries: List[str] = Field(description=\"List of sub-queries for comprehensive analysis\", \n",
    "                                   example=[\"What is the population of New York?\", \"What is the GDP of New York?\"])\n",
    "\n",
    "class AnalyticalRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def __init__(self, db, model_name=\"llama3\"):\n",
    "        self.db = db\n",
    "        self.llm = OllamaLLM(model=model_name)\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 4) -> List:\n",
    "        print(\"retrieving analytical\")\n",
    "\n",
    "        # Step 1: Generate sub-queries for comprehensive analysis\n",
    "        sub_queries_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"k\"],\n",
    "            template=\"Generate {k} sub-questions for: {query}\"\n",
    "        )\n",
    "        sub_queries_output = self.llm.invoke(sub_queries_prompt.format(query=query, k=k), output_class=SubQueries)\n",
    "        sub_queries = sub_queries_output.sub_queries\n",
    "        print(f'Sub-queries for comprehensive analysis: {sub_queries}')\n",
    "\n",
    "        # Step 2: Retrieve documents for each sub-query\n",
    "        all_docs = []\n",
    "        for sub_query in sub_queries:\n",
    "            all_docs.extend(self.db.similarity_search(sub_query, k=2))\n",
    "\n",
    "        # Step 3: Select diverse and relevant documents\n",
    "        diversity_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"docs\", \"k\"],\n",
    "            template=\"\"\"Select the most diverse and relevant set of {k} documents for the query: '{query}'.\n",
    "Return only the indices of selected documents as a list of integers.\n",
    "Documents: {docs}\"\"\"\n",
    "        )\n",
    "\n",
    "        docs_text = \"\\n\".join([f\"{i}: {doc.page_content[:50]}...\" for i, doc in enumerate(all_docs)])\n",
    "        selected_output = self.llm.invoke(\n",
    "            diversity_prompt.format(query=query, docs=docs_text, k=k),\n",
    "            output_class=SelectedIndices\n",
    "        )\n",
    "        selected_indices = [i for i in selected_output.indices if i < len(all_docs)]\n",
    "        print(f'Selected diverse and relevant documents: {selected_indices}')\n",
    "\n",
    "        return [all_docs[i] for i in selected_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Opinion retriever strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define structured output for selected indices\n",
    "class SelectedIndices(BaseModel):\n",
    "    indices: str = Field(description=\"Space-separated indices of selected documents\")\n",
    "\n",
    "class OpinionRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def __init__(self, db, model_name=\"llama3\"):\n",
    "        \"\"\"\n",
    "        db: vector database\n",
    "        model_name: Ollama LLM model name\n",
    "        \"\"\"\n",
    "        self.db = db\n",
    "        self.llm = OllamaLLM(model=model_name)\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 3) -> List:\n",
    "        print(\"retrieving opinion\")\n",
    "\n",
    "        # Step 1: Generate viewpoints using Ollama LLM\n",
    "        viewpoints_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"k\"],\n",
    "            template=\"Identify {k} distinct viewpoints or perspectives on the topic: {query}\"\n",
    "        )\n",
    "        input_data = {\"query\": query, \"k\": k}\n",
    "        viewpoints_text = self.llm.invoke(viewpoints_prompt.format(**input_data)).content\n",
    "        viewpoints = [vp.strip() for vp in viewpoints_text.split('\\n') if vp.strip()]\n",
    "        print(f'viewpoints: {viewpoints}')\n",
    "\n",
    "        # Step 2: Retrieve documents for each viewpoint\n",
    "        all_docs = []\n",
    "        for viewpoint in viewpoints:\n",
    "            all_docs.extend(self.db.similarity_search(f\"{query} {viewpoint}\", k=2))\n",
    "\n",
    "        # Step 3: Classify and select diverse opinions\n",
    "        opinion_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"docs\", \"k\"],\n",
    "            template=(\n",
    "                \"Classify these documents into distinct opinions on '{query}' \"\n",
    "                \"and select the {k} most representative and diverse viewpoints:\\n\"\n",
    "                \"Documents: {docs}\\nSelected indices:\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        docs_text = \"\\n\".join([f\"{i}: {doc.page_content[:100]}...\" for i, doc in enumerate(all_docs)])\n",
    "        input_data = {\"query\": query, \"docs\": docs_text, \"k\": k}\n",
    "\n",
    "        # Use Ollama LLM with structured output\n",
    "        selected_output = self.llm.invoke(opinion_prompt.format(**input_data), output_class=SelectedIndices)\n",
    "        selected_indices = [int(i) for i in selected_output.indices.split() if i.isdigit() and int(i) < len(all_docs)]\n",
    "\n",
    "        print(f'selected diverse and relevant documents: {selected_indices}')\n",
    "\n",
    "        return [all_docs[i] for i in selected_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Contextual retriever strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RelevanceScore(BaseModel):\n",
    "    score: float = Field(description=\"Relevance score of the document from 1 to 10\")\n",
    "\n",
    "class ContextualRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def __init__(self, db, model_name=\"llama3\"):\n",
    "        \"\"\"\n",
    "        db: vector database\n",
    "        model_name: Ollama model name\n",
    "        \"\"\"\n",
    "        self.db = db\n",
    "        self.llm = OllamaLLM(model=model_name)\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 4, user_context: str = None) -> List:\n",
    "        print(\"retrieving contextual\")\n",
    "\n",
    "        # Step 1: Contextualize the query using Ollama LLM\n",
    "        context_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"context\"],\n",
    "            template=\"Given the user context: {context}\\nReformulate the query to best address the user's needs: {query}\"\n",
    "        )\n",
    "        input_data = {\"query\": query, \"context\": user_context or \"No specific context provided\"}\n",
    "        contextualized_query = self.llm.invoke(context_prompt.format(**input_data)).content\n",
    "        print(f'contextualized query: {contextualized_query}')\n",
    "\n",
    "        # Step 2: Retrieve documents using the contextualized query\n",
    "        docs = self.db.similarity_search(contextualized_query, k=k*2)\n",
    "\n",
    "        # Step 3: Rank documents using Ollama LLM with structured output\n",
    "        ranking_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"context\", \"doc\"],\n",
    "            template=\"Given the query: '{query}' and user context: '{context}', rate the relevance of this document on a scale of 1-10:\\nDocument: {doc}\\nRelevance score:\"\n",
    "        )\n",
    "\n",
    "        ranked_docs = []\n",
    "        for doc in docs:\n",
    "            input_data = {\n",
    "                \"query\": contextualized_query,\n",
    "                \"context\": user_context or \"No specific context provided\",\n",
    "                \"doc\": doc.page_content\n",
    "            }\n",
    "            # Use Ollama LLM with Pydantic schema for structured output\n",
    "            score_output = self.llm.invoke(ranking_prompt.format(**input_data), output_class=RelevanceScore)\n",
    "            ranked_docs.append((doc, score_output.score))\n",
    "\n",
    "        # Step 4: Sort by relevance score and return top-k\n",
    "        ranked_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [doc for doc, _ in ranked_docs[:k]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Adapive retriever class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveRetriever:\n",
    "    def __init__(self, texts: List[str]):\n",
    "        self.classifier = QueryClassifier()\n",
    "        self.strategies = {\n",
    "            \"Factual\": FactualRetrievalStrategy(texts),\n",
    "            \"Analytical\": AnalyticalRetrievalStrategy(texts),\n",
    "            \"Opinion\": OpinionRetrievalStrategy(texts),\n",
    "            \"Contextual\": ContextualRetrievalStrategy(texts)\n",
    "        }\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        category = self.classifier.classify(query)\n",
    "        strategy = self.strategies[category]\n",
    "        return strategy.retrieve(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define aditional retriever that inherits from langchain BaseRetriever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bx/b60shh3d5fgcy2bhfzqsdnn00000gn/T/ipykernel_9703/4016186069.py:1: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n",
      "  class PydanticAdaptiveRetriever(BaseRetriever):\n",
      "/var/folders/bx/b60shh3d5fgcy2bhfzqsdnn00000gn/T/ipykernel_9703/4016186069.py:1: DeprecationWarning: Retrievers must implement abstract `_aget_relevant_documents` method instead of `aget_relevant_documents`\n",
      "  class PydanticAdaptiveRetriever(BaseRetriever):\n"
     ]
    }
   ],
   "source": [
    "class PydanticAdaptiveRetriever(BaseRetriever):\n",
    "    adaptive_retriever: AdaptiveRetriever = Field(exclude=True)\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        return self.adaptive_retriever.get_relevant_documents(query)\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        return self.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Adaptive RAG class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_t_with_space(list_of_documents):\n",
    "    \"\"\"\n",
    "    Replaces all tab characters ('\\t') with spaces in the page content of each document\n",
    "\n",
    "    Args:\n",
    "        list_of_documents: A list of document objects, each with a 'page_content' attribute.\n",
    "\n",
    "    Returns:\n",
    "        The modified list of documents with tab characters replaced by spaces.\n",
    "    \"\"\"\n",
    "\n",
    "    for doc in list_of_documents:\n",
    "        doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces\n",
    "    return list_of_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveRAG:\n",
    "    def __init__(self, texts: List[str]):\n",
    "        adaptive_retriever = AdaptiveRetriever(texts)\n",
    "        self.retriever = PydanticAdaptiveRetriever(adaptive_retriever=adaptive_retriever)\n",
    "        self.llm = OllamaLLM(model=\"llama3\")\n",
    "        \n",
    "        # Create a custom prompt\n",
    "        prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "        Answer:\"\"\"\n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "        \n",
    "        # Create the LLM chain\n",
    "        self.llm_chain = prompt | self.llm\n",
    "        \n",
    "      \n",
    "\n",
    "    def answer(self, query: str) -> str:\n",
    "        docs = self.retriever.get_relevant_documents(query)\n",
    "        input_data = {\"context\": \"\\n\".join([doc.page_content for doc in docs]), \"question\": query}\n",
    "        return self.llm_chain.invoke(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate use of this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "texts = [\n",
    "    \"The Earth is the third planet from the Sun and the only astronomical object known to harbor life.\"\n",
    "    ]\n",
    "rag_system = AdaptiveRAG(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showcase the four different types of queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying query...\n",
      "retrieving factual\n",
      "Enhanced query: To enhance this factual query, we can make it more specific and detailed to retrieve more accurate and relevant information. Here are some suggestions:\n",
      "\n",
      "1. **Specificity**: Instead of asking about the \"distance\" between the two bodies, you could ask about the **average distance**, which is a well-defined value (about 149.6 million kilometers or 92.96 million miles).\n",
      "2. **Unit**: Specify the unit of measurement you're interested in. For example: \"What is the average distance between Earth and Sun in kilometers?\" or \"What is the average distance between Earth and Sun in astronomical units?\"\n",
      "3. **Context**: Consider adding context to your query to help narrow down the results. For instance, you could ask about the:\n",
      "\t* Perihelion (closest point) or aphelion (farthest point) of the Earth's orbit around the Sun.\n",
      "\t* Distance at a specific time or date (e.g., \"What is the distance between Earth and Sun on June 21st?\")\n",
      "4. **Additional information**: You could also ask related questions to gain a better understanding of the topic. For example:\n",
      "\t* What is the elliptical shape of the Earth's orbit around the Sun, and how does it affect the distance?\n",
      "\t* How does the distance between Earth and Sun vary throughout the year?\n",
      "\n",
      "Here are some examples of enhanced factual queries:\n",
      "\n",
      "1. \"What is the average distance between Earth and Sun in kilometers?\"\n",
      "2. \"What is the perihelion (closest point) of the Earth's orbit around the Sun, and how does it compare to the aphelion (farthest point)?\"\n",
      "3. \"What is the distance between Earth and Sun on June 21st, and how does it relate to the Earth's elliptical orbit?\"\n",
      "4. \"How does the varying distance between Earth and Sun affect the intensity of sunlight on our planet?\"\n",
      "\n",
      "By asking more specific and detailed questions, you'll be more likely to retrieve accurate and relevant information from your search engine or database.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'similarity_search'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m factual_result \u001b[38;5;241m=\u001b[39m \u001b[43mrag_system\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manswer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the distance between the Earth and the Sun?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfactual_result\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m analytical_result \u001b[38;5;241m=\u001b[39m rag_system\u001b[38;5;241m.\u001b[39manswer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow does the Earth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms distance from the Sun affect its climate?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcontent\n",
      "Cell \u001b[0;32mIn[36], line 23\u001b[0m, in \u001b[0;36mAdaptiveRAG.answer\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21manswer\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 23\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     input_data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: query}\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39minvoke(input_data)\n",
      "File \u001b[0;32m~/Desktop/RAG_Technique/myenv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:190\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    189\u001b[0m     emit_warning()\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/RAG_Technique/myenv/lib/python3.12/site-packages/langchain_core/retrievers.py:414\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[0;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_name:\n\u001b[1;32m    413\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_name\n\u001b[0;32m--> 414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/RAG_Technique/myenv/lib/python3.12/site-packages/langchain_core/retrievers.py:265\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\n\u001b[1;32m    262\u001b[0m             \u001b[38;5;28minput\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_\n\u001b[1;32m    263\u001b[0m         )\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    267\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n",
      "Cell \u001b[0;32mIn[19], line 8\u001b[0m, in \u001b[0;36mPydanticAdaptiveRetriever.get_relevant_documents\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_relevant_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madaptive_retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 14\u001b[0m, in \u001b[0;36mAdaptiveRetriever.get_relevant_documents\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     12\u001b[0m category \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier\u001b[38;5;241m.\u001b[39mclassify(query)\n\u001b[1;32m     13\u001b[0m strategy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategies[category]\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 21\u001b[0m, in \u001b[0;36mFactualRetrievalStrategy.retrieve\u001b[0;34m(self, query, k)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnhanced query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menhanced_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Step 2: Retrieve documents using the enhanced query\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search\u001b[49m(enhanced_query, k\u001b[38;5;241m=\u001b[39mk\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Step 3: Rank documents using the LLM\u001b[39;00m\n\u001b[1;32m     24\u001b[0m ranking_prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[1;32m     25\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     26\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOn a scale of 1-10, how relevant is this document to the query: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{query}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDocument: \u001b[39m\u001b[38;5;132;01m{doc}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRelevance score:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'similarity_search'"
     ]
    }
   ],
   "source": [
    "factual_result = rag_system.answer(\"What is the distance between the Earth and the Sun?\").content\n",
    "print(f\"Answer: {factual_result}\")\n",
    "\n",
    "analytical_result = rag_system.answer(\"How does the Earth's distance from the Sun affect its climate?\").content\n",
    "print(f\"Answer: {analytical_result}\")\n",
    "\n",
    "opinion_result = rag_system.answer(\"What are the different theories about the origin of life on Earth?\").content\n",
    "print(f\"Answer: {opinion_result}\")\n",
    "\n",
    "contextual_result = rag_system.answer(\"How does the Earth's position in the Solar System influence its habitability?\").content\n",
    "print(f\"Answer: {contextual_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://europe-west1-rag-techniques-views-tracker.cloudfunctions.net/rag-techniques-tracker?notebook=all-rag-techniques--adaptive-retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying query...\n",
      "Query category: Factual\n",
      "Retrieving factual documents...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama import OllamaLLM, OllamaEmbeddings\n",
    "\n",
    "# ----------------------------\n",
    "# Query Classifier\n",
    "# ----------------------------\n",
    "class QueryClassifier:\n",
    "    def __init__(self, model_name=\"llama3\"):\n",
    "        self.llm = OllamaLLM(model=model_name)\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"query\"],\n",
    "            template=(\n",
    "                \"Classify the following query into one of these categories: \"\n",
    "                \"Factual, Analytical, Opinion, or Contextual.\\nQuery: {query}\\nAnswer only with the category:\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def classify(self, query: str) -> str:\n",
    "        print(\"Classifying query...\")\n",
    "        formatted_prompt = self.prompt.format(query=query)\n",
    "        raw_output = self.llm.invoke(formatted_prompt)\n",
    "        # Extract only the first matching category\n",
    "        for cat in [\"Factual\", \"Analytical\", \"Opinion\", \"Contextual\"]:\n",
    "            if cat.lower() in raw_output.lower():\n",
    "                return cat\n",
    "        raise ValueError(f\"Could not classify query. LLM output: {raw_output}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Base Retrieval Strategy\n",
    "# ----------------------------\n",
    "class BaseRetrievalStrategy:\n",
    "    def __init__(self, texts: List[str]):\n",
    "        self.embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=0)\n",
    "        self.documents = text_splitter.create_documents(texts)\n",
    "        self.db = FAISS.from_documents(self.documents, self.embeddings)\n",
    "        self.llm = OllamaLLM(model=\"llama3\")\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 4) -> List[Document]:\n",
    "        return self.db.similarity_search(query, k=k)\n",
    "\n",
    "# ----------------------------\n",
    "# Factual Strategy\n",
    "# ----------------------------\n",
    "class FactualRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def retrieve(self, query: str, k: int = 4) -> List[Document]:\n",
    "        print(\"Retrieving factual documents...\")\n",
    "        # Step 1: Enhance query\n",
    "        prompt = f\"Enhance this factual query for better information retrieval: {query}\"\n",
    "        enhanced_query = self.llm.invoke(prompt).strip()\n",
    "        print(f\"Enhanced query: {enhanced_query}\")\n",
    "\n",
    "        # Step 2: Retrieve\n",
    "        docs = self.db.similarity_search(enhanced_query, k=k*2)\n",
    "\n",
    "        # Step 3: Rank documents\n",
    "        ranking_prompt_template = \"On a scale of 1-10, how relevant is this document to the query: '{query}'?\\nDocument: {doc}\\nRelevance score:\"\n",
    "        ranked_docs = []\n",
    "        for doc in docs:\n",
    "            input_text = ranking_prompt_template.format(query=enhanced_query, doc=doc.page_content)\n",
    "            score_str = self.llm.invoke(input_text).strip()\n",
    "            try:\n",
    "                score = float(score_str)\n",
    "            except ValueError:\n",
    "                score = 0.0\n",
    "            ranked_docs.append((doc, score))\n",
    "\n",
    "        # Step 4: Return top-k\n",
    "        ranked_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [doc for doc, _ in ranked_docs[:k]]\n",
    "\n",
    "# ----------------------------\n",
    "# Analytical Strategy\n",
    "# ----------------------------\n",
    "class AnalyticalRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def retrieve(self, query: str, k: int = 4) -> List[Document]:\n",
    "        print(\"Retrieving analytical documents...\")\n",
    "        # Step 1: Generate sub-queries\n",
    "        prompt = f\"Generate {k} sub-questions for: {query}\"\n",
    "        sub_queries_text = self.llm.invoke(prompt).strip()\n",
    "        sub_queries = [sq.strip() for sq in sub_queries_text.split(\"\\n\") if sq.strip()]\n",
    "        print(f\"Sub-queries: {sub_queries}\")\n",
    "\n",
    "        # Step 2: Retrieve for each sub-query\n",
    "        all_docs = []\n",
    "        for sq in sub_queries:\n",
    "            all_docs.extend(self.db.similarity_search(sq, k=2))\n",
    "\n",
    "        # Step 3: Select top k manually (just take first k for simplicity)\n",
    "        return all_docs[:k]\n",
    "\n",
    "# ----------------------------\n",
    "# Opinion Strategy\n",
    "# ----------------------------\n",
    "class OpinionRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def retrieve(self, query: str, k: int = 3) -> List[Document]:\n",
    "        print(\"Retrieving opinion documents...\")\n",
    "        # Step 1: Generate viewpoints\n",
    "        prompt = f\"Identify {k} distinct viewpoints or perspectives on the topic: {query}\"\n",
    "        viewpoints_text = self.llm.invoke(prompt).strip()\n",
    "        viewpoints = [vp.strip() for vp in viewpoints_text.split(\"\\n\") if vp.strip()]\n",
    "        print(f\"Viewpoints: {viewpoints}\")\n",
    "\n",
    "        # Step 2: Retrieve for each viewpoint\n",
    "        all_docs = []\n",
    "        for vp in viewpoints:\n",
    "            all_docs.extend(self.db.similarity_search(f\"{query} {vp}\", k=2))\n",
    "\n",
    "        # Step 3: Return first k\n",
    "        return all_docs[:k]\n",
    "\n",
    "# ----------------------------\n",
    "# Contextual Strategy\n",
    "# ----------------------------\n",
    "class ContextualRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def retrieve(self, query: str, k: int = 4, user_context: str = None) -> List[Document]:\n",
    "        print(\"Retrieving contextual documents...\")\n",
    "        context_prompt = f\"Given the user context: {user_context or 'No specific context provided'}\\nReformulate the query: {query}\"\n",
    "        contextual_query = self.llm.invoke(context_prompt).strip()\n",
    "        print(f\"Contextualized query: {contextual_query}\")\n",
    "\n",
    "        docs = self.db.similarity_search(contextual_query, k=k*2)\n",
    "        return docs[:k]\n",
    "\n",
    "# ----------------------------\n",
    "# Adaptive Retriever\n",
    "# ----------------------------\n",
    "class AdaptiveRetriever:\n",
    "    def __init__(self, texts: List[str]):\n",
    "        self.classifier = QueryClassifier()\n",
    "        self.strategies = {\n",
    "            \"Factual\": FactualRetrievalStrategy(texts),\n",
    "            \"Analytical\": AnalyticalRetrievalStrategy(texts),\n",
    "            \"Opinion\": OpinionRetrievalStrategy(texts),\n",
    "            \"Contextual\": ContextualRetrievalStrategy(texts)\n",
    "        }\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        category = self.classifier.classify(query)\n",
    "        print(f\"Query category: {category}\")\n",
    "        strategy = self.strategies[category]\n",
    "        return strategy.retrieve(query)\n",
    "\n",
    "# ----------------------------\n",
    "# Adaptive RAG System\n",
    "# ----------------------------\n",
    "class AdaptiveRAG:\n",
    "    def __init__(self, texts: List[str]):\n",
    "        self.retriever = AdaptiveRetriever(texts)\n",
    "        self.llm = OllamaLLM(model=\"llama3\")\n",
    "        prompt_template = \"\"\"Use the following context to answer the question.\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "        self.prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "    def answer(self, query: str) -> str:\n",
    "        docs = self.retriever.get_relevant_documents(query)\n",
    "        context_text = \"\\n\".join([doc.page_content for doc in docs])\n",
    "        input_data = self.prompt.format(context=context_text, question=query)\n",
    "        return self.llm.invoke(input_data).strip()\n",
    "\n",
    "# ----------------------------\n",
    "# Usage Example\n",
    "# ----------------------------\n",
    "texts = [\n",
    "    \"The Earth is the third planet from the Sun and the only astronomical object known to harbor life.\"\n",
    "]\n",
    "\n",
    "rag_system = AdaptiveRAG(texts)\n",
    "\n",
    "# Factual\n",
    "factual_result = rag_system.answer(\"What is the distance between the Earth and the Sun?\")\n",
    "print(\"Factual answer:\", factual_result)\n",
    "\n",
    "# Analytical\n",
    "analytical_result = rag_system.answer(\"How does the Earth's distance from the Sun affect its climate?\")\n",
    "print(\"Analytical answer:\", analytical_result)\n",
    "\n",
    "# Opinion\n",
    "opinion_result = rag_system.answer(\"What are the different theories about the origin of life on Earth?\")\n",
    "print(\"Opinion answer:\", opinion_result)\n",
    "\n",
    "# Contextual\n",
    "contextual_result = rag_system.answer(\"How does the Earth's position in the Solar System influence its habitability?\")\n",
    "print(\"Contextual answer:\", contextual_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
