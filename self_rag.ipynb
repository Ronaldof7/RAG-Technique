{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4cf11626-d8aa-46b5-b8f0-e781267a6dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from pydantic import BaseModel, Field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e63e797-7b14-4aed-a585-d966efbc448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ad77ac5b-de33-4970-a468-95a5da489415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_t_with_space(list_of_documents):\n",
    "    \"\"\"\n",
    "    Replaces all tab characters ('\\t') with spaces in the page content of each document\n",
    "\n",
    "    Args:\n",
    "        list_of_documents: A list of document objects, each with a 'page_content' attribute.\n",
    "\n",
    "    Returns:\n",
    "        The modified list of documents with tab characters replaced by spaces.\n",
    "    \"\"\"\n",
    "\n",
    "    for doc in list_of_documents:\n",
    "        doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces\n",
    "    return list_of_documents\n",
    "\n",
    "def encode_pdf(path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Encodes a PDF book into a vector store using OpenAI embeddings.\n",
    "\n",
    "    Args:\n",
    "        path: The path to the PDF file.\n",
    "        chunk_size: The desired size of each text chunk.\n",
    "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        A FAISS vector store containing the encoded book content.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load PDF documents\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "    )\n",
    "    \n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    cleaned_texts = replace_t_with_space(texts)\n",
    "\n",
    "    # Create embeddings and vector store\n",
    "    embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    vectorstore = FAISS.from_documents(cleaned_texts, embedding_model)\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "vectorstore = encode_pdf(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2e912316-c79a-4ad6-b68f-3a61c9e1edcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "class RetrievalResponse(BaseModel):\n",
    "    response: str = Field(..., description=\"Output only 'Yes' or 'No'.\")\n",
    "\n",
    "# Setup parser\n",
    "parser = PydanticOutputParser(pydantic_object=RetrievalResponse)\n",
    "\n",
    "# Prompt with format instructions\n",
    "retrieval_prompt = PromptTemplate(\n",
    "                                    input_variables=[\"query\"],\n",
    "                                    template=(\n",
    "                                        \"Given the query '{query}', determine if retrieval is necessary.\\n\"\n",
    "                                        \"Output only 'Yes' or 'No'.\\n{format_instructions}\"\n",
    "                                    ),\n",
    "                                    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    "                                )\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3\")\n",
    "\n",
    "def run_retrieval_chain(query: str) -> RetrievalResponse:\n",
    "    prompt = retrieval_prompt.format(query=query)\n",
    "    output = llm.invoke(prompt)\n",
    "    return parser.parse(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "411b100e-812b-40ad-b07d-e67c77f479ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GenerationResponse(BaseModel):\n",
    "    response: str = Field(..., description=\"The generated response.\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=GenerationResponse)\n",
    "\n",
    "generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\"],\n",
    "    template=(\n",
    "        \"Given the query:\\n{query}\\n\\n\"\n",
    "        \"and the context:\\n{context}\\n\\n\"\n",
    "        \"Generate a helpful response.\\n\"\n",
    "        \"{format_instructions}\"\n",
    "    ),\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3\")\n",
    "\n",
    "def run_generation_chain(query: str, context: str) -> GenerationResponse:\n",
    "    prompt = generation_prompt.format(query=query, context=context)\n",
    "    output = llm.invoke(prompt)\n",
    "    return parser.parse(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "616282cf-e8be-4c4c-ac2c-18d279ac16f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelevanceResponse(BaseModel):\n",
    "    response: str = Field(..., description=\"Output only 'Relevant' or 'Irrelevant'.\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=RelevanceResponse)\n",
    "\n",
    "relevance_prompt = PromptTemplate(\n",
    "                        input_variables=[\"query\", \"context\"],\n",
    "                        template=\"Given the query '{query}' and the context '{context}', determine if the context is relevant. Output only 'relevant' or 'irrelevant' only.\"\n",
    "                        )\n",
    "\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3\")\n",
    "\n",
    "def run_relevance_chain(query: str, context: str) -> RelevanceResponse:\n",
    "    \"\"\"Run relevance checking chain.\"\"\"\n",
    "    prompt = relevance_prompt.format(query=query, context=context)\n",
    "    output = llm.invoke(prompt)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a3a6391d-602f-4726-a65d-db4b6af9b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SupportResponse(BaseModel):\n",
    "    response: str = Field(..., description=\"Output 'Fully supported', 'Partially supported', or 'No support'.\")\n",
    "\n",
    "support_parser = PydanticOutputParser(pydantic_object=SupportResponse)\n",
    "\n",
    "support_prompt = PromptTemplate(\n",
    "    input_variables=[\"response\", \"context\"],\n",
    "    template=\"\"\"\n",
    "You are a strict classifier. \n",
    "Given the response '{response}' and the context '{context}', determine if the response is supported. \n",
    "\n",
    "Return ONLY a valid JSON object, nothing else, in this exact format:\n",
    "\n",
    "{{\"response\": \"Fully supported\"}}\n",
    "OR\n",
    "{{\"response\": \"Partially supported\"}}\n",
    "OR\n",
    "{{\"response\": \"No support\"}}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "class UtilityResponse(BaseModel):\n",
    "    response: int = Field(..., description=\"Rate the utility of the response from 1 to 5.\")\n",
    "\n",
    "utility_parser = PydanticOutputParser(pydantic_object=UtilityResponse)\n",
    "\n",
    "utility_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"response\"],\n",
    "    template=\"\"\"\n",
    "You are a strict evaluator. \n",
    "Given the query '{query}' and the response '{response}', rate the utility of the response. \n",
    "\n",
    "Return ONLY a valid JSON object, nothing else, in this exact format:\n",
    "\n",
    "{{\"response\": 1}} OR {{\"response\": 2}} OR {{\"response\": 3}} OR {{\"response\": 4}} OR {{\"response\": 5}}\n",
    "\"\"\"\n",
    ")\n",
    "llm = OllamaLLM(model=\"llama3\")\n",
    "\n",
    "def run_support(response: str, context: str):\n",
    "    prompt = support_prompt.format(response=response, context=context)\n",
    "    raw_output = llm.invoke(prompt)\n",
    "    try:\n",
    "        return support_parser.parse(raw_output)\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Parse error, raw output:\", raw_output)\n",
    "        return {\"response\": \"No support\"}  # fallback\n",
    "\n",
    "def run_utility(query: str, response: str):\n",
    "    prompt = utility_prompt.format(query=query, response=response)\n",
    "    raw_output = llm.invoke(prompt)\n",
    "    try:\n",
    "        return utility_parser.parse(raw_output)\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Parse error, raw output:\", raw_output)\n",
    "        return {\"response\": 3}  # fallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c8a1f667-4324-4fb3-8783-6cd564e5b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_rag(query, vectorstore, top_k=3):\n",
    "    print(f\"\\nProcessing query: {query}\")\n",
    "    \n",
    "    # Step 1: Determine if retrieval is necessary\n",
    "    print(\"Step 1: Determining if retrieval is necessary...\")\n",
    "    retrieval_decision = run_retrieval_chain(query).response.strip().lower()\n",
    "    print(f\"Retrieval decision: {retrieval_decision}\")\n",
    "\n",
    "    if retrieval_decision == 'yes':\n",
    "        # Step 2: Retrieve relevant documents\n",
    "        print(\"Step 2: Retrieving relevant documents...\")\n",
    "        docs = vectorstore.similarity_search(query, k=top_k)\n",
    "        contexts = [doc.page_content for doc in docs]\n",
    "        print(f\"Retrieved {len(contexts)} documents\")\n",
    "\n",
    "        # Step 3: Evaluate relevance of retrieved documents\n",
    "        print(\"Step 3: Evaluating relevance of retrieved documents...\")\n",
    "        relevant_contexts = []\n",
    "        for i, context in enumerate(contexts):\n",
    "            relevance = run_relevance_chain(query, context)\n",
    "            print(f\"Document {i+1} relevance: {relevance}\")\n",
    "            if relevance.lower() == 'relevant':\n",
    "                relevant_contexts.append(context)\n",
    "\n",
    "        print(f\"Number of relevant contexts: {len(relevant_contexts)}\")\n",
    "        # If no relevant contexts found, generate without retrieval\n",
    "        if not relevant_contexts:\n",
    "            print(\"No relevant contexts found. Generating without retrieval...\")\n",
    "            input_data = {\"query\": query, \"context\": \"No relevant context found.\"}\n",
    "            return generation_chain.invoke(input_data).response\n",
    "\n",
    "        # Step 4: Generate response using relevant contexts\n",
    "        print(\"Step 4: Generating responses using relevant contexts...\")\n",
    "        responses = []\n",
    "        for i, context in enumerate(relevant_contexts):\n",
    "            print(f\"Generating response for context {i+1}...\")\n",
    "            input_data = {\"query\": query, \"context\": context}\n",
    "            response = run_generation_chain(query, context)\n",
    "            \n",
    "            # Step 5: Assess support\n",
    "            print(f\"Step 5: Assessing support for response {i+1}...\")\n",
    "            input_data = {\"response\": response, \"context\": context}\n",
    "            support = run_support(response, context)\n",
    "            print(f\"Support assessment: {support}\")\n",
    "            \n",
    "            # Step 6: Evaluate utility\n",
    "            print(f\"Step 6: Evaluating utility for response {i+1}...\")\n",
    "            utility = run_utility(query, response)\n",
    "            print(f\"Utility score: {utility}\")\n",
    "            \n",
    "            responses.append((response, support, utility))\n",
    "\n",
    "        # Select the best response based on support and utility\n",
    "        print(\"Selecting the best response...\")\n",
    "        best_response = max(responses, key=lambda x: (x[1] == 'fully supported', x[2]))\n",
    "        print(f\"Best response support: {best_response[1]}, utility: {best_response[2]}\")\n",
    "        return best_response[0]\n",
    "            \n",
    "    else:\n",
    "        # Generate without retrieval\n",
    "        print(\"Generating without retrieval...\")\n",
    "        return run_generation_chain(query, \"No retrieval necessary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0aac7eff-79cf-4768-8187-45e633f1f3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query: What is the impact of climate change on the environment?\n",
      "Step 1: Determining if retrieval is necessary...\n",
      "Retrieval decision: yes\n",
      "Step 2: Retrieving relevant documents...\n",
      "Retrieved 3 documents\n",
      "Step 3: Evaluating relevance of retrieved documents...\n",
      "Document 1 relevance: Relevant\n",
      "Document 2 relevance: Relevant\n",
      "Document 3 relevance: Relevant\n",
      "Number of relevant contexts: 3\n",
      "Step 4: Generating responses using relevant contexts...\n",
      "Generating response for context 1...\n",
      "Step 5: Assessing support for response 1...\n",
      "Support assessment: response='Fully supported'\n",
      "Step 6: Evaluating utility for response 1...\n",
      "Utility score: response=5\n",
      "Generating response for context 2...\n",
      "Step 5: Assessing support for response 2...\n",
      "Support assessment: response='Fully supported'\n",
      "Step 6: Evaluating utility for response 2...\n",
      "Utility score: response=5\n",
      "Generating response for context 3...\n",
      "Step 5: Assessing support for response 3...\n",
      "Support assessment: response='Fully supported'\n",
      "Step 6: Evaluating utility for response 3...\n",
      "Utility score: response=5\n",
      "Selecting the best response...\n",
      "Best response support: response='Fully supported', utility: response=5\n",
      "\n",
      "Final response:\n",
      "response=\"Climate change has a significant impact on the environment. Rising temperatures, heatwaves, and changing seasons are just a few examples of its effects. Rising temperatures have led to melting glaciers and sea-level rise, while heatwaves pose risks to human health, agriculture, and infrastructure. Changing seasons can disrupt ecosystems and human activities. To mitigate these impacts, developing eco-friendly fertilizers and farming techniques is essential for reducing the agricultural sector's carbon footprint.\"\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the impact of climate change on the environment?\"\n",
    "response = self_rag(query, vectorstore)\n",
    "\n",
    "print(\"\\nFinal response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a45d24f-4a8a-40af-b864-fa3470ee6c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d14efc1-f2b9-4bf0-812d-fb47d3eafae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
