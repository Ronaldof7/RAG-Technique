{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import AIMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model\n",
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "# LLM model\n",
    "llm = OllamaLLM(model=\"llama3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(item):\n",
    "    \"\"\"Extract text content from either a string or an AIMessage object.\"\"\"\n",
    "    if isinstance(item, AIMessage):\n",
    "        return item.content\n",
    "    return item\n",
    "\n",
    "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "    \"\"\"Embed texts using OpenAIEmbeddings.\"\"\"\n",
    "    print(f\"Embedding {len(texts)} texts\")\n",
    "    return embedding_model.embed_documents([extract_text(text) for text in texts])\n",
    "\n",
    "def perform_clustering(embeddings: np.ndarray, n_clusters: int = 10) -> np.ndarray:\n",
    "    \"\"\"Perform clustering on embeddings using Gaussian Mixture Model.\"\"\"\n",
    "    print(f\"Performing clustering with {n_clusters} clusters\")\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=42)\n",
    "    return gm.fit_predict(embeddings)\n",
    "\n",
    "def summarize_texts(texts: List[str]) -> str:\n",
    "    \"\"\"Summarize a list of texts using OpenAI.\"\"\"\n",
    "    print(f\"Summarizing {len(texts)} texts\")\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Summarize the following text concisely:\\n\\n{text}\"\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "    input_data = {\"text\": texts}\n",
    "    return chain.invoke(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAPTOR Core Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_raptor_tree(texts: List[str], max_levels: int = 3) -> Dict[int, pd.DataFrame]:\n",
    "    \"\"\"Build the RAPTOR tree structure with level metadata and parent-child relationships.\"\"\"\n",
    "    results = {}\n",
    "    current_texts = [extract_text(text) for text in texts]\n",
    "    current_metadata = [{\"level\": 0, \"origin\": \"original\", \"parent_id\": None} for _ in texts]\n",
    "    \n",
    "    for level in range(1, max_levels + 1):\n",
    "        print(f\"Processing level {level}\")\n",
    "        \n",
    "        embeddings = embed_texts(current_texts)\n",
    "        n_clusters = min(10, len(current_texts) // 2)\n",
    "        cluster_labels = perform_clustering(np.array(embeddings), n_clusters)\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'text': current_texts,\n",
    "            'embedding': embeddings,\n",
    "            'cluster': cluster_labels,\n",
    "            'metadata': current_metadata\n",
    "        })\n",
    "        \n",
    "        results[level-1] = df\n",
    "        \n",
    "        summaries = []\n",
    "        new_metadata = []\n",
    "        for cluster in df['cluster'].unique():\n",
    "            cluster_docs = df[df['cluster'] == cluster]\n",
    "            cluster_texts = cluster_docs['text'].tolist()\n",
    "            cluster_metadata = cluster_docs['metadata'].tolist()\n",
    "            summary = summarize_texts(cluster_texts)\n",
    "            summaries.append(summary)\n",
    "            new_metadata.append({\n",
    "                \"level\": level,\n",
    "                \"origin\": f\"summary_of_cluster_{cluster}_level_{level-1}\",\n",
    "                \"child_ids\": [meta.get('id') for meta in cluster_metadata],\n",
    "                \"id\": f\"summary_{level}_{cluster}\"\n",
    "            })\n",
    "        \n",
    "        current_texts = summaries\n",
    "        current_metadata = new_metadata\n",
    "        \n",
    "        if len(current_texts) <= 1:\n",
    "            results[level] = pd.DataFrame({\n",
    "                'text': current_texts,\n",
    "                'embedding': embed_texts(current_texts),\n",
    "                'cluster': [0],\n",
    "                'metadata': current_metadata\n",
    "            })\n",
    "            print(f\"Stopping at level {level} as we have only one summary\")\n",
    "            break\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_vectorstore(tree_results: Dict[int, pd.DataFrame]) -> FAISS:\n",
    "    \"\"\"Build a FAISS vectorstore from all texts in the RAPTOR tree.\"\"\"\n",
    "    all_texts = []\n",
    "    all_embeddings = []\n",
    "    all_metadatas = []\n",
    "    \n",
    "    for level, df in tree_results.items():\n",
    "        all_texts.extend([str(text) for text in df['text'].tolist()])\n",
    "        all_embeddings.extend([embedding.tolist() if isinstance(embedding, np.ndarray) else embedding for embedding in df['embedding'].tolist()])\n",
    "        all_metadatas.extend(df['metadata'].tolist())\n",
    "\n",
    "    # Create Document objects manually to ensure correct types\n",
    "    documents = [Document(page_content=str(text), metadata=metadata) \n",
    "                 for text, metadata in zip(all_texts, all_metadatas)]\n",
    "\n",
    "    # print(\"\\n Example Document object:\")\n",
    "    # print(documents[1])\n",
    "    return FAISS.from_documents(documents, embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ContextualCompressionRetriever:\n",
    "# It‚Äôs a wrapper retriever that improves the quality of retrieved documents by compressing (shortening / filtering) them before returning.\n",
    "    \n",
    "def create_retriever(vectorstore: FAISS) -> ContextualCompressionRetriever:\n",
    "    \"\"\"Create a retriever with contextual compression.\"\"\"\n",
    "    logging.info(\"Creating contextual compression retriever\")\n",
    "    base_retriever = vectorstore.as_retriever()\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Given the following context and question, extract only the relevant information for answering the question:\\n\\n\"\n",
    "        \"Context: {context}\\n\"\n",
    "        \"Question: {question}\\n\\n\"\n",
    "        \"Relevant Information:\"\n",
    "    )\n",
    "    \n",
    "    extractor = LLMChainExtractor.from_llm(llm, prompt=prompt)\n",
    "    \n",
    "    return ContextualCompressionRetriever(\n",
    "        base_compressor=extractor,\n",
    "        base_retriever=base_retriever\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAPTOR Query Process (Online Process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_retrieval(query: str, retriever: ContextualCompressionRetriever, max_level: int) -> List[Document]:\n",
    "    \"\"\"Perform hierarchical retrieval starting from the highest level, handling potential None values.\"\"\"\n",
    "    all_retrieved_docs = []\n",
    "    for level in range(max_level, -1, -1):\n",
    "        # Retrieve documents from the current level\n",
    "        level_docs = retriever.get_relevant_documents(\n",
    "            query,\n",
    "            filter=lambda meta: meta['level'] == level\n",
    "        )\n",
    "        all_retrieved_docs.extend(level_docs)\n",
    "\n",
    "    # If we've found documents, retrieve their children from the next level down\n",
    "        if level_docs and level > 0:\n",
    "            child_ids = [doc.metadata.get('child_ids', []) for doc in level_docs]\n",
    "            child_ids = [item for sublist in child_ids for item in sublist if item is not None]  # Flatten and filter None\n",
    "            \n",
    "            if child_ids:  # Only modify query if there are valid child IDs\n",
    "                child_query = f\" AND id:({' OR '.join(str(id) for id in child_ids)})\"\n",
    "                query += child_query\n",
    "    \n",
    "    return all_retrieved_docs\n",
    "    \n",
    "def raptor_query(query: str, retriever: ContextualCompressionRetriever, max_level: int) -> Dict[str, Any]:\n",
    "    \"\"\"Process a query using the RAPTOR system with hierarchical retrieval.\"\"\"\n",
    "    logging.info(f\"Processing query: {query}\")\n",
    "\n",
    "    relevant_docs = hierarchical_retrieval(query, retriever, max_level)\n",
    "\n",
    "    doc_details = []\n",
    "    for i, doc in enumerate(relevant_docs, 1):\n",
    "        doc_details.append({\n",
    "            \"index\": i,\n",
    "            \"content\": doc.page_content,\n",
    "            \"metadata\": doc.metadata,\n",
    "            \"level\": doc.metadata.get('level', 'Unknown'),\n",
    "            \"similarity_score\": doc.metadata.get('score', 'N/A')\n",
    "        })\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Given the following context, please answer the question:\\n\\n\"\n",
    "        \"Context: {context}\\n\\n\"\n",
    "        \"Question: {question}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    answer = chain.run(context=context, question=query)\n",
    "    \n",
    "    logging.info(\"Query processing completed\")\n",
    "    \n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"retrieved_documents\": doc_details,\n",
    "        \"num_docs_retrieved\": len(relevant_docs),\n",
    "        \"context_used\": context,\n",
    "        \"answer\": answer,\n",
    "        \"model_used\": \"llama3\",\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "def print_query_details(result: Dict[str, Any]):\n",
    "    \"\"\"Print detailed information about the query process, including tree level metadata.\"\"\"\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"\\nNumber of documents retrieved: {result['num_docs_retrieved']}\")\n",
    "    print(f\"\\nRetrieved Documents:\")\n",
    "    for doc in result['retrieved_documents']:\n",
    "        print(f\"  Document {doc['index']}:\")\n",
    "        print(f\"    Content: {doc['content'][:100]}...\")  # Show first 100 characters\n",
    "        print(f\"    Similarity Score: {doc['similarity_score']}\")\n",
    "        print(f\"    Tree Level: {doc['metadata'].get('level', 'Unknown')}\")\n",
    "        print(f\"    Origin: {doc['metadata'].get('origin', 'Unknown')}\")\n",
    "        if 'child_docs' in doc['metadata']:\n",
    "            print(f\"    Number of Child Documents: {len(doc['metadata']['child_docs'])}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"\\nContext used for answer generation:\")\n",
    "    print(result['context_used'])\n",
    "    \n",
    "    print(f\"\\nGenerated Answer:\")\n",
    "    print(result['answer'])\n",
    "    \n",
    "    print(f\"\\nModel Used: {result['model_used']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(path)\n",
    "documents = loader.load()\n",
    "texts = [doc.page_content for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the RAPTOR tree\n",
    "tree_results = build_raptor_tree(texts)\n",
    "\n",
    "# # Write tree_results dict to a text file\n",
    "# with open(\"tree_results.txt\", \"w\") as f:\n",
    "#     for level, df in tree_results.items():\n",
    "#         f.write(f\"--- Level {level} ---\\n\")\n",
    "#         f.write(df.to_string(index=False))  # write DataFrame as string\n",
    "#         f.write(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vectorstore\n",
    "vectorstore = build_vectorstore(tree_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever\n",
    "retriever = create_retriever(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the Innovative Adaptation Strategies?\n",
      "\n",
      "Number of documents retrieved: 4\n",
      "\n",
      "Retrieved Documents:\n",
      "  Document 1:\n",
      "    Content: According to the context, the Innovative Adaptation Strategies mentioned are:\n",
      "\n",
      "1. Climate-resilient ...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 2:\n",
      "    Content: Based on the provided context, the relevant information for answering the question \"What are the Inn...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 3:\n",
      "    Content: The relevant information for answering the question is:\n",
      "\n",
      "* Community-based solutions (e.g., communit...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 4:\n",
      "    Content: According to the context, there is no mention of \"Innovative Adaptation Strategies\". However, some r...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "\n",
      "Context used for answer generation:\n",
      "According to the context, the Innovative Adaptation Strategies mentioned are:\n",
      "\n",
      "1. Climate-resilient agriculture: This involves adopting practices that enhance the ability of farming systems to withstand climate impacts, including diversifying crops, improving soil health, and using water-efficient irrigation techniques.\n",
      "2. Water Management: Effective strategies include enhancing water storage, improving distribution systems, and promoting water conservation.\n",
      "3. None mentioned explicitly in this context (although urban resilience is discussed separately).\n",
      "\n",
      "These are the relevant answers for the question about Innovative Adaptation Strategies.\n",
      "\n",
      "Based on the provided context, the relevant information for answering the question \"What are the Innovative Adaptation Strategies?\" is:\n",
      "\n",
      "* Nature-Based Solutions:\n",
      "\t+ Ecosystem Restoration\n",
      "\t+ Sustainable Land Management\n",
      "\t+ Coastal Protection\n",
      "\n",
      "These strategies involve using natural systems and processes to mitigate climate-related risks and promote resilience.\n",
      "\n",
      "The relevant information for answering the question is:\n",
      "\n",
      "* Community-based solutions (e.g., community-led renewable energy projects, urban gardening, local conservation efforts)\n",
      "* Innovative Financing Models (e.g., green bonds, impact investing, crowdfunding)\n",
      "* Adaptive Governance\n",
      "* Participatory Governance (e.g., citizen assemblies, participatory budgeting, co-management of natural resources)\n",
      "* Collaborative Platforms (e.g., digital tools and online networks)\n",
      "\n",
      "These strategies are referred to as \"Innovative Adaptation Strategies\" in the context.\n",
      "\n",
      "According to the context, there is no mention of \"Innovative Adaptation Strategies\". However, some relevant information that may be related to adaptation strategies can be found in the following:\n",
      "\n",
      "* Local Initiatives: Urban planning, public transportation improvements, and community-based conservation are mentioned as local initiatives for driving change at the local level.\n",
      "* Research and Innovation: Continuous research and innovation are vital for developing new technologies and strategies to combat climate change. This includes advancements in renewable energy, carbon capture and storage, and sustainable agriculture.\n",
      "\n",
      "Note that these points may not directly answer the question about \"Innovative Adaptation Strategies\", but they do mention some relevant information related to climate action and adaptation.\n",
      "\n",
      "Generated Answer:\n",
      "Based on the provided context, there is no explicit mention of \"Innovative Adaptation Strategies\". The relevant information mentions different strategies such as Nature-Based Solutions, Community-based solutions, Innovative Financing Models, Adaptive Governance, Participatory Governance, and Collaborative Platforms. However, these are not referred to as \"Innovative Adaptation Strategies\" in the context.\n",
      "\n",
      "So, the correct answer is:\n",
      "\n",
      "There is no mention of \"Innovative Adaptation Strategies\".\n",
      "\n",
      "Model Used: llama3\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline\n",
    "max_level = 3  # Adjust based on your tree depth\n",
    "query = \"What are the Innovative Adaptation Strategies?\"\n",
    "result = raptor_query(query, retriever, max_level)\n",
    "print_query_details(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Deciding Tree Length in RAPTOR Implementations\n",
    "\n",
    "In RAPTOR (hierarchical chunking + embeddings), **tree length** (depth) determines how many levels of summaries you create.\n",
    "\n",
    "---\n",
    "\n",
    "### üîé What Tree Length Means\n",
    "- **Leaf level** ‚Üí raw text chunks (e.g., 500‚Äì1000 tokens).  \n",
    "- **Intermediate levels** ‚Üí summaries of sibling chunks.  \n",
    "- **Root level** ‚Üí one summary representing the entire document.  \n",
    "\n",
    "The **tree depth** defines how many summarization layers exist above the raw chunks.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öñÔ∏è How to Decide Tree Length\n",
    "1. **Document size**\n",
    "   - Small (<5k tokens) ‚Üí Depth **1‚Äì2**\n",
    "   - Medium (5k‚Äì50k tokens) ‚Üí Depth **2‚Äì3**\n",
    "   - Large (>100k tokens) ‚Üí Depth **3‚Äì4**\n",
    "\n",
    "2. **Retrieval needs**\n",
    "   - Fine-grained retrieval ‚Üí Shallower (2 levels)  \n",
    "   - Broader semantic context ‚Üí Deeper (3‚Äì4 levels)  \n",
    "\n",
    "3. **Compute vs. Storage**\n",
    "   - More depth = More LLM calls + More embeddings.  \n",
    "   - Rarely useful beyond **depth = 4**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Rule of Thumb\n",
    "- Depth = **2** ‚Üí Enough for most RAG use cases  \n",
    "- Depth = **3** ‚Üí For large hierarchical docs (books, legal, multi-chapter reports)  \n",
    "- Depth > **4** ‚Üí Usually not worth the cost  \n",
    "\n",
    "---\n",
    "\n",
    "### üêç Example: Dynamic Depth Selection in Python\n",
    "\n",
    "```python\n",
    "def decide_tree_depth(token_count: int) -> int:\n",
    "    \"\"\"\n",
    "    Decide tree depth for RAPTOR-style hierarchical embeddings\n",
    "    based on document token size.\n",
    "    \"\"\"\n",
    "    if token_count < 5_000:\n",
    "        return 2   # chunks + summaries\n",
    "    elif token_count < 50_000:\n",
    "        return 3   # chunks + mid-level + root\n",
    "    else:\n",
    "        return 4   # very large docs\n",
    "\n",
    "# Example usage\n",
    "doc_tokens = 32000\n",
    "depth = decide_tree_depth(doc_tokens)\n",
    "print(f\"Recommended tree depth: {depth}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
