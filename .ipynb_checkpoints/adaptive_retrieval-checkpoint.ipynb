{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![](https://europe-west1-atp-views-tracker.cloudfunctions.net/working-analytics?notebook=adaptive-retrieval) -->\n",
    "\n",
    "\n",
    "\n",
    "# Adaptive Retrieval-Augmented Generation (RAG) System\n",
    "\n",
    "## Overview\n",
    "\n",
    "This system implements an advanced Retrieval-Augmented Generation (RAG) approach that adapts its retrieval strategy based on the type of query. By leveraging Language Models (LLMs) at various stages, it aims to provide more accurate, relevant, and context-aware responses to user queries.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Traditional RAG systems often use a one-size-fits-all approach to retrieval, which can be suboptimal for different types of queries. Our adaptive system is motivated by the understanding that different types of questions require different retrieval strategies. For example, a factual query might benefit from precise, focused retrieval, while an analytical query might require a broader, more diverse set of information.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "1. **Query Classifier**: Determines the type of query (Factual, Analytical, Opinion, or Contextual).\n",
    "\n",
    "2. **Adaptive Retrieval Strategies**: Four distinct strategies tailored to different query types:\n",
    "   - Factual Strategy\n",
    "   - Analytical Strategy\n",
    "   - Opinion Strategy\n",
    "   - Contextual Strategy\n",
    "\n",
    "3. **LLM Integration**: LLMs are used throughout the process to enhance retrieval and ranking.\n",
    "\n",
    "4. **OpenAI GPT Model**: Generates the final response using the retrieved documents as context.\n",
    "\n",
    "## Method Details\n",
    "\n",
    "### 1. Query Classification\n",
    "\n",
    "The system begins by classifying the user's query into one of four categories:\n",
    "- Factual: Queries seeking specific, verifiable information.\n",
    "- Analytical: Queries requiring comprehensive analysis or explanation.\n",
    "- Opinion: Queries about subjective matters or seeking diverse viewpoints.\n",
    "- Contextual: Queries that depend on user-specific context.\n",
    "\n",
    "### 2. Adaptive Retrieval Strategies\n",
    "\n",
    "Each query type triggers a specific retrieval strategy:\n",
    "\n",
    "#### Factual Strategy\n",
    "- Enhances the original query using an LLM for better precision.\n",
    "- Retrieves documents based on the enhanced query.\n",
    "- Uses an LLM to rank documents by relevance.\n",
    "\n",
    "#### Analytical Strategy\n",
    "- Generates multiple sub-queries using an LLM to cover different aspects of the main query.\n",
    "- Retrieves documents for each sub-query.\n",
    "- Ensures diversity in the final document selection using an LLM.\n",
    "\n",
    "#### Opinion Strategy\n",
    "- Identifies different viewpoints on the topic using an LLM.\n",
    "- Retrieves documents representing each viewpoint.\n",
    "- Uses an LLM to select a diverse range of opinions from the retrieved documents.\n",
    "\n",
    "#### Contextual Strategy\n",
    "- Incorporates user-specific context into the query using an LLM.\n",
    "- Performs retrieval based on the contextualized query.\n",
    "- Ranks documents considering both relevance and user context.\n",
    "\n",
    "### 3. LLM-Enhanced Ranking\n",
    "\n",
    "After retrieval, each strategy uses an LLM to perform a final ranking of the documents. This step ensures that the most relevant and appropriate documents are selected for the next stage.\n",
    "\n",
    "### 4. Response Generation\n",
    "\n",
    "The final set of retrieved documents is passed to an OpenAI GPT model, which generates a response based on the query and the provided context.\n",
    "\n",
    "## Benefits of This Approach\n",
    "\n",
    "1. **Improved Accuracy**: By tailoring the retrieval strategy to the query type, the system can provide more accurate and relevant information.\n",
    "\n",
    "2. **Flexibility**: The system adapts to different types of queries, handling a wide range of user needs.\n",
    "\n",
    "3. **Context-Awareness**: Especially for contextual queries, the system can incorporate user-specific information for more personalized responses.\n",
    "\n",
    "4. **Diverse Perspectives**: For opinion-based queries, the system actively seeks out and presents multiple viewpoints.\n",
    "\n",
    "5. **Comprehensive Analysis**: The analytical strategy ensures a thorough exploration of complex topics.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This adaptive RAG system represents a significant advancement over traditional RAG approaches. By dynamically adjusting its retrieval strategy and leveraging LLMs throughout the process, it aims to provide more accurate, relevant, and nuanced responses to a wide variety of user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/adaptive_retrieval.svg\" alt=\"adaptive retrieval\" style=\"width:100%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Installation and Imports\n",
    "\n",
    "The cell below installs all necessary packages required to run this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in ./myenv/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: langchain in ./myenv/lib/python3.12/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-openai in ./myenv/lib/python3.12/site-packages (0.3.6)\n",
      "Requirement already satisfied: python-dotenv in ./myenv/lib/python3.12/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in ./myenv/lib/python3.12/site-packages (from faiss-cpu) (2.1.3)\n",
      "Requirement already satisfied: packaging in ./myenv/lib/python3.12/site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in ./myenv/lib/python3.12/site-packages (from langchain) (0.3.75)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in ./myenv/lib/python3.12/site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in ./myenv/lib/python3.12/site-packages (from langchain) (0.4.21)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./myenv/lib/python3.12/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./myenv/lib/python3.12/site-packages (from langchain) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in ./myenv/lib/python3.12/site-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./myenv/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./myenv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./myenv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./myenv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./myenv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./myenv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./myenv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./myenv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in ./myenv/lib/python3.12/site-packages (from langchain-openai) (1.63.2)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in ./myenv/lib/python3.12/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./myenv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./myenv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./myenv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./myenv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in ./myenv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./myenv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: httpcore==1.* in ./myenv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./myenv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./myenv/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2025.9.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in ./myenv/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./myenv/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./myenv/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install faiss-cpu langchain langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from typing import Dict, Any, List\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install \\\n",
    "#   langchain==0.2.16 \\\n",
    "#   langchain-core==0.2.38 \\\n",
    "#   langchain-community==0.2.16 \\\n",
    "#   langchain-ollama==0.1.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the query classifer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Query Classifier\n",
    "# ----------------------------\n",
    "class QueryClassifier:\n",
    "    def __init__(self, model_name=\"llama3\"):\n",
    "        self.llm = OllamaLLM(model=model_name)\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"query\"],\n",
    "            template=(\n",
    "                \"Classify the following query into one of these categories: \"\n",
    "                \"Factual, Analytical, Opinion, or Contextual.\\nQuery: {query}\\nAnswer only with the category:\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def classify(self, query: str) -> str:\n",
    "        print(\"Classifying query...\")\n",
    "        formatted_prompt = self.prompt.format(query=query)\n",
    "        raw_output = self.llm.invoke(formatted_prompt)\n",
    "        # Extract only the first matching category\n",
    "        for cat in [\"Factual\", \"Analytical\", \"Opinion\", \"Contextual\"]:\n",
    "            if cat.lower() in raw_output.lower():\n",
    "                return cat\n",
    "        raise ValueError(f\"Could not classify query. LLM output: {raw_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Base Retriever class, such that the complex ones will inherit from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Base Retrieval Strategy\n",
    "# ----------------------------\n",
    "class BaseRetrievalStrategy:\n",
    "    def __init__(self, texts: List[str]):\n",
    "        self.embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=0)\n",
    "        self.documents = text_splitter.create_documents(texts)\n",
    "        self.db = FAISS.from_documents(self.documents, self.embeddings)\n",
    "        self.llm = OllamaLLM(model=\"llama3\")\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 4) -> List[Document]:\n",
    "        return self.db.similarity_search(query, k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Factual retriever strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Factual Strategy\n",
    "# ----------------------------\n",
    "class FactualRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def retrieve(self, query: str, k: int = 4) -> List[Document]:\n",
    "        print(\"Retrieving factual documents...\")\n",
    "        # Step 1: Enhance query\n",
    "        prompt = f\"Enhance this factual query for better information retrieval: {query}\"\n",
    "        enhanced_query = self.llm.invoke(prompt).strip()\n",
    "        print(f\"Enhanced query: {enhanced_query}\")\n",
    "\n",
    "        # Step 2: Retrieve\n",
    "        docs = self.db.similarity_search(enhanced_query, k=k*2)\n",
    "\n",
    "        # Step 3: Rank documents\n",
    "        ranking_prompt_template = \"On a scale of 1-10, how relevant is this document to the query: '{query}'?\\nDocument: {doc}\\nRelevance score:\"\n",
    "        ranked_docs = []\n",
    "        for doc in docs:\n",
    "            input_text = ranking_prompt_template.format(query=enhanced_query, doc=doc.page_content)\n",
    "            score_str = self.llm.invoke(input_text).strip()\n",
    "            try:\n",
    "                score = float(score_str)\n",
    "            except ValueError:\n",
    "                score = 0.0\n",
    "            ranked_docs.append((doc, score))\n",
    "\n",
    "        # Step 4: Return top-k\n",
    "        ranked_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [doc for doc, _ in ranked_docs[:k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Analytical retriever strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Analytical Strategy\n",
    "# ----------------------------\n",
    "class AnalyticalRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def retrieve(self, query: str, k: int = 4) -> List[Document]:\n",
    "        print(\"Retrieving analytical documents...\")\n",
    "        # Step 1: Generate sub-queries\n",
    "        prompt = f\"Generate {k} sub-questions for: {query}\"\n",
    "        sub_queries_text = self.llm.invoke(prompt).strip()\n",
    "        sub_queries = [sq.strip() for sq in sub_queries_text.split(\"\\n\") if sq.strip()]\n",
    "        print(f\"Sub-queries: {sub_queries}\")\n",
    "\n",
    "        # Step 2: Retrieve for each sub-query\n",
    "        all_docs = []\n",
    "        for sq in sub_queries:\n",
    "            all_docs.extend(self.db.similarity_search(sq, k=2))\n",
    "\n",
    "        # Step 3: Select top k manually (just take first k for simplicity)\n",
    "        return all_docs[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Opinion retriever strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Opinion Strategy\n",
    "# ----------------------------\n",
    "class OpinionRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def retrieve(self, query: str, k: int = 3) -> List[Document]:\n",
    "        print(\"Retrieving opinion documents...\")\n",
    "        # Step 1: Generate viewpoints\n",
    "        prompt = f\"Identify {k} distinct viewpoints or perspectives on the topic: {query}\"\n",
    "        viewpoints_text = self.llm.invoke(prompt).strip()\n",
    "        viewpoints = [vp.strip() for vp in viewpoints_text.split(\"\\n\") if vp.strip()]\n",
    "        print(f\"Viewpoints: {viewpoints}\")\n",
    "\n",
    "        # Step 2: Retrieve for each viewpoint\n",
    "        all_docs = []\n",
    "        for vp in viewpoints:\n",
    "            all_docs.extend(self.db.similarity_search(f\"{query} {vp}\", k=2))\n",
    "\n",
    "        # Step 3: Return first k\n",
    "        return all_docs[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Contextual retriever strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Contextual Strategy\n",
    "# ----------------------------\n",
    "class ContextualRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def retrieve(self, query: str, k: int = 4, user_context: str = None) -> List[Document]:\n",
    "        print(\"Retrieving contextual documents...\")\n",
    "        context_prompt = f\"Given the user context: {user_context or 'No specific context provided'}\\nReformulate the query: {query}\"\n",
    "        contextual_query = self.llm.invoke(context_prompt).strip()\n",
    "        print(f\"Contextualized query: {contextual_query}\")\n",
    "\n",
    "        docs = self.db.similarity_search(contextual_query, k=k*2)\n",
    "        return docs[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Adapive retriever class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Adaptive Retriever\n",
    "# ----------------------------\n",
    "class AdaptiveRetriever:\n",
    "    def __init__(self, texts: List[str]):\n",
    "        self.classifier = QueryClassifier()\n",
    "        self.strategies = {\n",
    "            \"Factual\": FactualRetrievalStrategy(texts),\n",
    "            \"Analytical\": AnalyticalRetrievalStrategy(texts),\n",
    "            \"Opinion\": OpinionRetrievalStrategy(texts),\n",
    "            \"Contextual\": ContextualRetrievalStrategy(texts)\n",
    "        }\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        category = self.classifier.classify(query)\n",
    "        print(f\"Query category: {category}\")\n",
    "        strategy = self.strategies[category]\n",
    "        return strategy.retrieve(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Adaptive RAG class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Adaptive RAG System\n",
    "# ----------------------------\n",
    "class AdaptiveRAG:\n",
    "    def __init__(self, texts: List[str]):\n",
    "        self.retriever = AdaptiveRetriever(texts)\n",
    "        self.llm = OllamaLLM(model=\"llama3\")\n",
    "        prompt_template = \"\"\"Use the following context to answer the question.\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "        self.prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "    def answer(self, query: str) -> str:\n",
    "        docs = self.retriever.get_relevant_documents(query)\n",
    "        context_text = \"\\n\".join([doc.page_content for doc in docs])\n",
    "        input_data = self.prompt.format(context=context_text, question=query)\n",
    "        return self.llm.invoke(input_data).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate use of this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying query...\n",
      "Query category: Factual\n",
      "Retrieving factual documents...\n",
      "Enhanced query: Here's an enhanced version of your factual query:\n",
      "\n",
      "\"What is the average distance between the center of the Earth and the center of the Sun, considering various methods of measurement and accounting for slight variations due to the elliptical shape of both bodies' orbits?\"\n",
      "\n",
      "This revised query aims to provide more specific and accurate information by:\n",
      "\n",
      "1. Specifying that you're looking for the average distance: This helps to distinguish from other queries that might be asking about the closest or farthest points in their orbits.\n",
      "2. Providing context about measurement methods: By mentioning various methods of measurement, you're giving search algorithms a better idea of what types of information are relevant to your query (e.g., orbital mechanics, astronomical units, etc.).\n",
      "3. Accounting for orbit shape: The Earth and Sun's orbits aren't perfect circles; they're elliptical, which means their distances from each other vary slightly throughout the year. By acknowledging this, you're giving search algorithms a chance to provide more nuanced answers that take into account these small variations.\n",
      "\n",
      "By asking a more specific and informative query, you'll be more likely to retrieve accurate and relevant information about the distance between Earth and Sun!\n",
      "Factual answer: According to the given context, it doesn't mention the distance between the Earth and the Sun. It only states that the Earth is the \"third planet from the Sun\", implying that there are two other planets closer to the Sun than the Earth, but it does not provide a specific distance.\n",
      "Classifying query...\n",
      "Query category: Analytical\n",
      "Retrieving analytical documents...\n",
      "Sub-queries: [\"Here are four sub-questions that can help to further explore the relationship between the Earth's distance from the Sun and its climate:\", '1. **How does the varying amount of solar energy received by the Earth due to its elliptical orbit influence global temperatures?**', \"This sub-question delves into the specific impact of the Earth's elliptical shape on the amount of solar energy it receives, and how this affects global temperatures.\", '2. **What role do changes in atmospheric circulation patterns play in mediating the effects of distance from the Sun on regional climate conditions?**', \"This sub-question investigates how changes in atmospheric circulation patterns (such as wind patterns or ocean currents) influence the distribution of heat around the globe, which is affected by the Earth's distance from the Sun.\", \"3. **How do differences in albedo (reflectivity) between Earth's surface and atmosphere affect the amount of solar energy absorbed by the planet at different distances from the Sun?**\", \"This sub-question examines how the reflectivity of the Earth's surface and atmosphere impacts the amount of solar energy absorbed, which is crucial for understanding how distance from the Sun affects climate.\", \"4. **What are the implications of orbital variations (such as those caused by Milankovitch cycles) on the Earth's distance from the Sun for long-term climate trends and variability?**\", \"This sub-question explores the relationship between changes in the Earth's orbit and its distance from the Sun, which can have significant impacts on global temperatures over thousands to millions of years.\"]\n",
      "Analytical answer: There is no information provided about how the Earth's distance from the Sun affects its climate. The context only mentions that the Earth is the third planet from the Sun and the only astronomical object known to harbor life, but it doesn't provide any details about the relationship between the Earth's distance from the Sun and its climate.\n",
      "Classifying query...\n",
      "Query category: Analytical\n",
      "Retrieving analytical documents...\n",
      "Sub-queries: ['Here are four potential sub-questions that could be explored in relation to \"What are the different theories about the origin of life on Earth?\"', '1. **What is the RNA World Hypothesis and how does it propose the emergence of life on Earth?**', 'The RNA World Hypothesis suggests that life began when RNA molecules, which can store genetic information and catalyze chemical reactions, played a central role in the process. This theory proposes that RNA-based replicators gave rise to the first cells.', '2. **How do iron-sulfur world theories propose the origin of life on Earth?**', 'Iron-sulfur world theories suggest that simple iron-sulfur compounds, which are rich in energy and chemical reactivity, played a key role in the emergence of life. These compounds could have facilitated the assembly of complex biomolecules and ultimately given rise to the first living cells.', '3. **What is the Primordial Soup Hypothesis, and how does it describe the conditions that led to the origin of life on Earth?**', 'The Primordial Soup Hypothesis proposes that life emerged from a mixture of organic compounds that existed in a primordial soup, rich in nutrients and energy sources. This theory suggests that simple molecules reacted and interacted to form more complex structures, eventually giving rise to the first cells.', '4. **How do panspermia theories propose the origin of life on Earth, and what are some criticisms of these ideas?**', 'Panspermia theories suggest that life on Earth originated from outside our planet, either through comets or meteorites carrying microbial life forms or through the transfer of genetic material between celestial bodies. While these ideas have gained attention, they also face significant scientific challenges and skepticism.']\n",
      "Opinion answer: There is no information provided in the context that discusses the different theories about the origin of life on Earth. The context simply states that the Earth is the third planet from the Sun and the only astronomical object known to harbor life, without providing any details about the origin of life. Therefore, it's not possible to answer this question based on the given context.\n",
      "Classifying query...\n",
      "Query category: Analytical\n",
      "Retrieving analytical documents...\n",
      "Sub-queries: ['Here are four potential sub-questions that could be explored to delve deeper into the main question:', \"1. **What is the optimal distance from the Sun for life to thrive on Earth, and how has this distance influenced the planet's climate and geology?**\", 'This sub-question could explore the concept of the \"Goldilocks zone\" and how Earth\\'s distance from the Sun affects its ability to support liquid water, atmospheric circulation, and other factors that contribute to habitability. It might also examine the geological consequences of being in this sweet spot, such as the planet\\'s surface temperature and the presence of oceans.', \"2. **How do the gravitational influences of nearby planets affect Earth's tides, ocean currents, and climate patterns?**\", \"This sub-question could investigate how the gravitational pulls of other planets, particularly Jupiter and Venus, impact Earth's tidal patterns, ocean circulation, and atmospheric circulation. It might also explore how these effects have shaped the planet's geology and influenced the development of life.\", \"3. **What role do solar flares and coronal mass ejections play in shaping Earth's magnetosphere and influencing habitability?**\", \"This sub-question could examine the impact of intense solar activity on Earth's magnetic field, which protects the planet from harmful radiation and charged particles. It might explore how these events have affected the development of life on Earth, including the evolution of complex organisms that rely on magnetic fields for navigation.\", \"4. **How do variations in the Solar System's internal dynamics, such as changes in planetary orbital patterns or solar oscillations, affect Earth's climate and habitability over geological timescales?**\", \"This sub-question could investigate how subtle changes in the Solar System's internal dynamics might influence Earth's climate and habitability over long periods of time. It might explore how these variations have shaped the planet's geological history, including the formation of ice ages or the development of complex ecosystems.\", \"These sub-questions can help provide a more nuanced understanding of the complex relationships between the Earth's position in the Solar System and its habitability.\"]\n",
      "Contextual answer: Based on the provided context, it can be inferred that the Earth's position as the third planet from the Sun plays no role in its habitability. The statement simply says that the Earth is the third planet and the only astronomical object known to harbor life, without mentioning anything about its position influencing its habitability.\n",
      "\n",
      "Therefore, the correct answer would be: \"The context does not provide any information about how the Earth's position influences its habitability.\"\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Usage Example\n",
    "# ----------------------------\n",
    "texts = [\n",
    "    \"The Earth is the third planet from the Sun and the only astronomical object known to harbor life.\"\n",
    "]\n",
    "\n",
    "rag_system = AdaptiveRAG(texts)\n",
    "\n",
    "# Factual\n",
    "factual_result = rag_system.answer(\"What is the distance between the Earth and the Sun?\")\n",
    "print(\"Factual answer:\", factual_result)\n",
    "\n",
    "# Analytical\n",
    "analytical_result = rag_system.answer(\"How does the Earth's distance from the Sun affect its climate?\")\n",
    "print(\"Analytical answer:\", analytical_result)\n",
    "\n",
    "# Opinion\n",
    "opinion_result = rag_system.answer(\"What are the different theories about the origin of life on Earth?\")\n",
    "print(\"Opinion answer:\", opinion_result)\n",
    "\n",
    "# Contextual\n",
    "contextual_result = rag_system.answer(\"How does the Earth's position in the Solar System influence its habitability?\")\n",
    "print(\"Contextual answer:\", contextual_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://europe-west1-rag-techniques-views-tracker.cloudfunctions.net/rag-techniques-tracker?notebook=all-rag-techniques--adaptive-retrieval)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
