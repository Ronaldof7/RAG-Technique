{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cf11626-d8aa-46b5-b8f0-e781267a6dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "from langchain.chains.summarize.chain import load_summarize_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55a814ca-873f-482c-a52b-fffcf445e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed1e1d1-ddd9-4baa-9f2b-dc7f66d7f103",
   "metadata": {},
   "source": [
    "Encode the pdf to vector store and return split document from the step before to create BM25 instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a12d639-fa71-49a0-b8f9-3f7238c1f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import random\n",
    "\n",
    "async def retry_with_exponential_backoff(\n",
    "    func, \n",
    "    max_retries: int = 5, \n",
    "    initial_delay: float = 1, \n",
    "    max_delay: float = 60, \n",
    "    jitter: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Retry a coroutine with exponential backoff.\n",
    "    \n",
    "    Args:\n",
    "        func: Coroutine function to retry.\n",
    "        max_retries: Maximum number of retry attempts.\n",
    "        initial_delay: Initial wait before first retry.\n",
    "        max_delay: Maximum wait time between retries.\n",
    "        jitter: Whether to add randomness to delay.\n",
    "\n",
    "    Returns:\n",
    "        The result of the coroutine if successful.\n",
    "    \"\"\"\n",
    "    delay = initial_delay\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return await func  # <-- func should already be awaited outside\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise  # re-raise final error\n",
    "            sleep_time = min(delay, max_delay)\n",
    "            if jitter:\n",
    "                sleep_time = sleep_time * (0.5 + random.random() / 2)\n",
    "            print(f\"Retry {attempt+1}/{max_retries} after {sleep_time:.2f}s due to error: {e}\")\n",
    "            await asyncio.sleep(sleep_time)\n",
    "            delay *= 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "502f1ea0-57c2-4a7d-9ade-37322bcbd755",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def encode_pdf_hierarchical(path, chunk_size=1000, chunk_overlap=200, is_string=False):\n",
    "    \"\"\"\n",
    "    Asynchronously encodes a PDF book into a hierarchical vector store using OpenAI embeddings.\n",
    "    Includes rate limit handling with exponential backoff.\n",
    "    \n",
    "    Args:\n",
    "        path: The path to the PDF file.\n",
    "        chunk_size: The desired size of each text chunk.\n",
    "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple containing two FAISS vector stores:\n",
    "        1. Document-level summaries\n",
    "        2. Detailed chunks\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load PDF documents\n",
    "    if not is_string:\n",
    "        loader = PyPDFLoader(path)\n",
    "        documents = await asyncio.to_thread(loader.load)\n",
    "    else:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            # Set a really small chunk size, just to show.\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "        documents = text_splitter.create_documents([path])\n",
    "\n",
    "    # Create document-level summaries\n",
    "    summary_llm = OllamaLLM(model=\"llama3\")\n",
    "    summary_chain = load_summarize_chain(summary_llm, chain_type=\"map_reduce\")\n",
    "\n",
    "    async def summarize_doc(doc):\n",
    "        \"\"\"\n",
    "        Summarizes a single document with rate limit handling.\n",
    "        \n",
    "        Args:\n",
    "            doc: The document to be summarized.\n",
    "            \n",
    "        Returns:\n",
    "            A summarized Document object.\n",
    "        \"\"\"\n",
    "        # Retry the summarization with exponential backoff\n",
    "        summary_output = await retry_with_exponential_backoff(summary_chain.ainvoke([doc]))\n",
    "        summary = summary_output['output_text']\n",
    "        return Document(\n",
    "            page_content=summary,\n",
    "            metadata={\"source\": path, \"page\": doc.metadata[\"page\"], \"summary\": True}\n",
    "        )\n",
    "\n",
    "    # Process documents in smaller batches to avoid rate limits\n",
    "    batch_size = 5  # Adjust this based on your rate limits\n",
    "    summaries = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        batch_summaries = await asyncio.gather(*[summarize_doc(doc) for doc in batch])\n",
    "        summaries.extend(batch_summaries)\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "    # Split documents into detailed chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "    )\n",
    "    detailed_chunks = await asyncio.to_thread(text_splitter.split_documents, documents)\n",
    "\n",
    "    # Update metadata for detailed chunks\n",
    "    for i, chunk in enumerate(detailed_chunks):\n",
    "        chunk.metadata.update({\n",
    "            \"chunk_id\": i,\n",
    "            \"summary\": False,\n",
    "            \"page\": int(chunk.metadata.get(\"page\", 0))\n",
    "        })\n",
    "\n",
    "    # Create embeddings\n",
    "    embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "    # Create vector stores asynchronously with rate limit handling\n",
    "    async def create_vectorstore(docs):\n",
    "        \"\"\"\n",
    "        Creates a vector store from a list of documents with rate limit handling.\n",
    "        \n",
    "        Args:\n",
    "            docs: The list of documents to be embedded.\n",
    "            \n",
    "        Returns:\n",
    "            A FAISS vector store containing the embedded documents.\n",
    "        \"\"\"\n",
    "        return await retry_with_exponential_backoff(\n",
    "            asyncio.to_thread(FAISS.from_documents, docs, embedding_model)\n",
    "        )\n",
    "\n",
    "    # Generate vector stores for summaries and detailed chunks concurrently\n",
    "    summary_vectorstore, detailed_vectorstore = await asyncio.gather(\n",
    "        create_vectorstore(summaries),\n",
    "        create_vectorstore(detailed_chunks)\n",
    "    )\n",
    "\n",
    "    return summary_vectorstore, detailed_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "839c0380-2c40-4e68-9b00-f84107897ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"../vector_stores/summary_store\") and os.path.exists(\"../vector_stores/detailed_store\"):\n",
    "   embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "   summary_store = FAISS.load_local(\"../vector_stores/summary_store\", embeddings, allow_dangerous_deserialization=True)\n",
    "   detailed_store = FAISS.load_local(\"../vector_stores/detailed_store\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "else:\n",
    "    summary_store, detailed_store = await encode_pdf_hierarchical(path)\n",
    "    summary_store.save_local(\"../vector_stores/summary_store\")\n",
    "    detailed_store.save_local(\"../vector_stores/detailed_store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac43f69-f551-4692-a772-8912cbcc444d",
   "metadata": {},
   "source": [
    "Retrieve information according to summary level, and then retrieve information from the chunk level vector store and filter according to the summary level pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd050b76-21ac-4367-9490-7a4123b1dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_hierarchical(query, summary_vectorstore, detailed_vectorstore, k_summaries=3, k_chunks=5):\n",
    "    \"\"\"\n",
    "    Performs a hierarchical retrieval using the query.\n",
    "\n",
    "    Args:\n",
    "        query: The search query.\n",
    "        summary_vectorstore: The vector store containing document summaries.\n",
    "        detailed_vectorstore: The vector store containing detailed chunks.\n",
    "        k_summaries: The number of top summaries to retrieve.\n",
    "        k_chunks: The number of detailed chunks to retrieve per summary.\n",
    "\n",
    "    Returns:\n",
    "        A list of relevant detailed chunks.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve top summaries\n",
    "    top_summaries = summary_vectorstore.similarity_search(query, k=k_summaries)\n",
    "    \n",
    "    relevant_chunks = []\n",
    "    for summary in top_summaries:\n",
    "        # For each summary, retrieve relevant detailed chunks\n",
    "        page_number = summary.metadata[\"page\"]\n",
    "        page_filter = lambda metadata: metadata[\"page\"] == page_number\n",
    "        page_chunks = detailed_vectorstore.similarity_search(\n",
    "            query, \n",
    "            k=k_chunks, \n",
    "            filter=page_filter\n",
    "        )\n",
    "        relevant_chunks.extend(page_chunks)\n",
    "    \n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "525d5442-33ba-4f8c-8f5f-646efd69f24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 1\n",
      "Content: Tropical rainforests are particularly important for carbon storage. Deforestation in the \n",
      "Amazon, Congo Basin, and Southeast Asia has significant impacts on global carbon cycles \n",
      "and biodiversity. These regions are often cleared for agriculture, logging, and mining, leading \n",
      "to habitat loss and species extinction. \n",
      "Boreal Forests \n",
      "Boreal forests, found in the northern regions of North America, Europe, and Asia, also play a \n",
      "crucial role in sequestering carbon. Logging and land-use changes in these regions contribute \n",
      "to climate change. These forests are vital for regulating the Earth's climate and supporting \n",
      "indigenous communities and wildlife. \n",
      "Agriculture \n",
      "Agriculture contributes to climate change through methane emissions from livestock, rice \n",
      "paddies, and the use of synthetic fertilizers. Methane is a potent greenhouse gas with a much \n",
      "higher heat-trapping capability than CO2, albeit in smaller quantities. \n",
      "Livestock Emissions...\n",
      "---\n",
      "Page: 1\n",
      "Content: Natural Gas \n",
      "Natural gas is the least carbon-intensive fossil fuel and is often seen as a \"bridge fuel\" to a \n",
      "lower-carbon future. However, its extraction and use still contribute to greenhouse gas \n",
      "emissions, particularly methane, which is a potent greenhouse gas. Innovations in fracking \n",
      "technology have made natural gas more accessible, but this comes with environmental and \n",
      "health concerns. \n",
      "Deforestation \n",
      "Forests act as carbon sinks, absorbing CO2 from the atmosphere. When trees are cut down \n",
      "for timber or to clear land for agriculture, this stored carbon is released back into the \n",
      "atmosphere. Deforestation reduces the number of trees that can absorb CO2, exacerbating the \n",
      "greenhouse effect. \n",
      "Tropical Deforestation \n",
      "Tropical rainforests are particularly important for carbon storage. Deforestation in the \n",
      "Amazon, Congo Basin, and Southeast Asia has significant impacts on global carbon cycles...\n",
      "---\n",
      "Page: 0\n",
      "Content: Most of these climate changes are attributed to very small variations in Earth's orbit that \n",
      "change the amount of solar energy our planet receives. During the Holocene epoch, which \n",
      "began at the end of the last ice age, human societies flourished, but the industrial era has seen \n",
      "unprecedented changes. \n",
      "Modern Observations \n",
      "Modern scientific observations indicate a rapid increase in global temperatures, sea levels, \n",
      "and extreme weather events. The Intergovernmental Panel on Climate Change (IPCC) has \n",
      "documented these changes extensively. Ice core samples, tree rings, and ocean sediments \n",
      "provide a historical record that scientists use to understand past climate conditions and \n",
      "predict future trends. The evidence overwhelmingly shows that recent changes are primarily \n",
      "driven by human activities, particularly the emission of greenhouse gases. \n",
      "Chapter 2: Causes of Climate Change \n",
      "Greenhouse Gases \n",
      "The primary cause of recent climate change is the increase in greenhouse gases in the...\n",
      "---\n",
      "Page: 0\n",
      "Content: Chapter 2: Causes of Climate Change \n",
      "Greenhouse Gases \n",
      "The primary cause of recent climate change is the increase in greenhouse gases in the \n",
      "atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous \n",
      "oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is essential \n",
      "for life on Earth, as it keeps the planet warm enough to support life. However, human \n",
      "activities have intensified this natural process, leading to a warmer climate. \n",
      "Fossil Fuels \n",
      "Burning fossil fuels for energy releases large amounts of CO2. This includes coal, oil, and \n",
      "natural gas used for electricity, heating, and transportation. The industrial revolution marked \n",
      "the beginning of a significant increase in fossil fuel consumption, which continues to rise \n",
      "today. \n",
      "Coal...\n",
      "---\n",
      "Page: 0\n",
      "Content: Understanding Climate Change \n",
      "Chapter 1: Introduction to Climate Change \n",
      "Climate change refers to significant, long-term changes in the global climate. The term \n",
      "\"global climate\" encompasses the planet's overall weather patterns, including temperature, \n",
      "precipitation, and wind patterns, over an extended period. Over the past century, human \n",
      "activities, particularly the burning of fossil fuels and deforestation, have significantly \n",
      "contributed to climate change. \n",
      "Historical Context \n",
      "The Earth's climate has changed throughout history. Over the past 650,000 years, there have \n",
      "been seven cycles of glacial advance and retreat, with the abrupt end of the last ice age about \n",
      "11,700 years ago marking the beginning of the modern climate era and human civilization. \n",
      "Most of these climate changes are attributed to very small variations in Earth's orbit that \n",
      "change the amount of solar energy our planet receives. During the Holocene epoch, which...\n",
      "---\n",
      "Page: 3\n",
      "Content: managed retreats. \n",
      "Extreme Weather Events \n",
      "Climate change is linked to an increase in the frequency and severity of extreme weather \n",
      "events, such as hurricanes, heatwaves, droughts, and heavy rainfall. These events can have \n",
      "devastating impacts on communities, economies, and ecosystems. \n",
      "Hurricanes and Typhoons \n",
      "Warmer ocean temperatures can intensify hurricanes and typhoons, leading to more \n",
      "destructive storms. Coastal regions are at heightened risk of storm surge and flooding. Early \n",
      "warning systems and resilient infrastructure are critical for mitigating these risks. \n",
      "Droughts \n",
      "Increased temperatures and changing precipitation patterns are contributing to more frequent \n",
      "and severe droughts. This affects agriculture, water supply, and ecosystems, particularly in \n",
      "arid and semi-arid regions. Droughts can lead to food and water shortages and exacerbate \n",
      "conflicts. \n",
      "Flooding \n",
      "Heavy rainfall events are becoming more common, leading to increased flooding. Urban...\n",
      "---\n",
      "Page: 3\n",
      "Content: The Arctic is warming at more than twice the global average rate, leading to significant ice \n",
      "loss. Antarctic ice sheets are also losing mass, contributing to sea level rise. This melting \n",
      "affects global ocean currents and weather patterns. \n",
      "Glacial Retreat \n",
      "Glaciers around the world are retreating, affecting water supplies for millions of people. \n",
      "Regions dependent on glacial meltwater, such as the Himalayas and the Andes, face \n",
      "particular risks. Glacial melt also impacts hydropower generation and agriculture. \n",
      "Coastal Erosion \n",
      "Rising sea levels and increased storm surges are accelerating coastal erosion, threatening \n",
      "homes, infrastructure, and ecosystems. Low-lying islands and coastal regions are especially \n",
      "vulnerable. Coastal communities must invest in adaptation measures like sea walls and \n",
      "managed retreats. \n",
      "Extreme Weather Events \n",
      "Climate change is linked to an increase in the frequency and severity of extreme weather...\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the greenhouse effect?\"\n",
    "results = retrieve_hierarchical(query, summary_store, detailed_store)\n",
    "\n",
    "# Print results\n",
    "for chunk in results:\n",
    "    print(f\"Page: {chunk.metadata['page']}\")\n",
    "    print(f\"Content: {chunk.page_content}...\")  # Print first 100 characters\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717fc63f-8b01-4d4d-a474-97dc668b177f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
